{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sbn\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import linear_model\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsRegressor as KNR\n",
    "from sklearn.metrics import mean_squared_error, r2_score,mean_absolute_error,make_scorer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import metrics\n",
    "from math import log\n",
    "\n",
    "sbn.set()\n",
    "\n",
    "def get_diff_month(d1,d2):\n",
    "    return (d1.year - d2.year) * 12 + d1.month - d2.month\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/properties_2016.csv',low_memory=False)\n",
    "original = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('./data/train_2016_v2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print len(df) #already checked for duplicate rows, the file doesn't have any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print df.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for d in df:\n",
    "#     print df[d].value_counts(dropna = False)\n",
    "print original['airconditioningtypeid'].describe()\n",
    "print original['airconditioningtypeid'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print len(df)\n",
    "# for d in original:\n",
    "#     print d, original[d].isnull().sum()\n",
    "# print merged['garagetotalsqft'].value_counts()#,merged['unitcnt'].value_counts()\n",
    "# pd.merge(original,train,on = 'parcelid')['garagetotalsqft'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "missing_data = original.isnull().sum()\n",
    "missing_data.sort_values(0,ascending=False,inplace = True)\n",
    "missing_data = missing_data/(len(original))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# missing_data\n",
    "missing_plot = missing_data.plot.bar(figsize= (12,7))\n",
    "plt.title('Percentage of missing values vs. properties in data')\n",
    "plt.xlabel('Properties of data')\n",
    "plt.ylabel('Percentage of missing values')\n",
    "plt.show()\n",
    "del missing_plot\n",
    "del missing_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that some variables have more than 99% of their data missing, using these variables for training might add noise to the data and hence adversely affect the training process "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "baseline_backup = original.copy()\n",
    "\n",
    "baseline_backup['yardbuildingsqft17'].fillna(0, inplace = True)\n",
    "baseline_backup['unitcnt'].fillna(1, inplace = True)\n",
    "baseline_backup['garagetotalsqft'].fillna(0, inplace = True)\n",
    "baseline_backup['fireplacecnt'].fillna(0, inplace = True)\n",
    "baseline_backup['yearbuilt'].fillna(0, inplace = True)\n",
    "baseline_backup['buildingqualitytypeid'].fillna(0, inplace = True)\n",
    "baseline_backup['bedroomcnt'].fillna(0, inplace = True)\n",
    "baseline_backup['lotsizesquarefeet'].fillna(0, inplace = True)\n",
    "baseline_backup['poolcnt'].fillna(0,inplace = True)\n",
    "baseline_backup['calculatedbathnbr'].fillna(0,inplace = True)\n",
    "baseline_backup['airconditioningtypeid'].fillna(0,inplace=True)\n",
    "\n",
    "#we cant keep object type data to use linear regression, so for the basline model, we drop those columns\n",
    "baseline_backup.drop(['architecturalstyletypeid','basementsqft', 'buildingclasstypeid', 'decktypeid', 'garagecarcnt',\n",
    "        'poolsizesum', 'pooltypeid10', 'pooltypeid2', 'pooltypeid7', 'roomcnt', 'yardbuildingsqft26', \n",
    "         'numberofstories', 'fireplaceflag', 'fullbathcnt', 'heatingorsystemtypeid','storytypeid', 'threequarterbathnbr', 'typeconstructiontypeid', \n",
    "         'structuretaxvaluedollarcnt', 'taxvaluedollarcnt', 'assessmentyear', 'landtaxvaluedollarcnt', \n",
    "         'taxamount', 'bathroomcnt','hashottuborspa','propertycountylandusecode','propertyzoningdesc','taxdelinquencyflag'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_merge = pd.merge(baseline_backup,train,on = 'parcelid')\n",
    "\n",
    "dates = []\n",
    "month = []\n",
    "for i in base_merge['transactiondate']:\n",
    "    dates.append(get_diff_month(datetime.strptime(i,'%Y-%m-%d'),datetime(2015,1,1)))\n",
    "\n",
    "base_merge['transactiondate'] = dates\n",
    "\n",
    "base_merge.fillna(0, inplace=True)  #fill all missing values with 0 in the dataframe\n",
    "\n",
    "\n",
    "base_logerror = base_merge['logerror']\n",
    "# invlogerror = [10**i for i in logerror]\n",
    "base_X_train, base_X_test, base_y_train, base_y_test = train_test_split(base_merge.drop(['logerror','parcelid'],axis=1), base_logerror, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_clf = LinearRegression(n_jobs=-1)\n",
    "base_clf.fit(base_X_train,base_y_train)\n",
    "\n",
    "pred = base_clf.predict(base_X_test)\n",
    "# res = [log(i,10) if i>0 else -log(i*-1,10) for i in res]\n",
    "print mean_absolute_error(base_y_test,pred), mean_squared_error(base_y_test,pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MSE is 0.0268 for the baseline model, in which we fill missing values in some of the columns, convert the transaction date to  a datetime variable  and use simple linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del base_X_train\n",
    "del base_X_test\n",
    "del base_y_train\n",
    "del base_y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['yardbuildingsqft17'].fillna(0, inplace = True)\n",
    "df['unitcnt'].fillna(1, inplace = True)\n",
    "df['garagetotalsqft'].fillna(0, inplace = True)\n",
    "df['fireplacecnt'].fillna(0, inplace = True)\n",
    "df['hashottuborspa'].fillna(0, inplace = True)\n",
    "df['yearbuilt'].fillna(df['yearbuilt'].median(), inplace = True)\n",
    "df['buildingqualitytypeid'].fillna(df['buildingqualitytypeid'].median(), inplace = True)\n",
    "df['bedroomcnt'].fillna(df['bedroomcnt'].median(), inplace = True)\n",
    "df['lotsizesquarefeet'].fillna(df['lotsizesquarefeet'].median(), inplace = True)\n",
    "df['poolcnt'].fillna(0,inplace = True)\n",
    "df['calculatedbathnbr'].fillna(df['calculatedbathnbr'].median(),inplace = True)\n",
    "df['airconditioningtypeid'].fillna(-1,inplace=True)  #earlier it was replaced with 0,now replacing it with -1\n",
    "\n",
    "df['actual_area'] = df[['finishedfloor1squarefeet','calculatedfinishedsquarefeet','finishedsquarefeet12', 'finishedsquarefeet13',\n",
    "                        'finishedsquarefeet15', 'finishedsquarefeet50', 'finishedsquarefeet6']].max(axis=1)\n",
    "\n",
    "df['hashottuborspa'] = df['hashottuborspa'].map({True:1,0:0})\n",
    "\n",
    "df['calculatedvalue'] = df[['structuretaxvaluedollarcnt', 'taxvaluedollarcnt', 'landtaxvaluedollarcnt', 'taxamount']].sum(axis=1)\n",
    "\n",
    "df.drop(['finishedfloor1squarefeet','calculatedfinishedsquarefeet','finishedsquarefeet12', 'finishedsquarefeet13',\n",
    "              'finishedsquarefeet15', 'finishedsquarefeet50', 'finishedsquarefeet6'],axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can drop 'finishedfloor1squarefeet','calculatedfinishedsquarefeet','finishedsquarefeet12', 'finishedsquarefeet13', 'finishedsquarefeet15', 'finishedsquarefeet50' and 'finishedsquarefeet6' because we marked the NaN values as 0, then took the max of all the columns and stored it in 'actual_area'\n",
    "\n",
    "We can also drop: \n",
    "'bathroomcnt', 'threequarterbathnbr', 'fullbathcnt' and 'threequarterbathnbr' because 'calculatedbathnbr' stores all this information combined in one column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "age = []\n",
    "for i in df['yearbuilt']:\n",
    "    age.append(int(2015-i))\n",
    "\n",
    "df['age'] = age\n",
    "df.drop(['yearbuilt'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "before_drop = df.copy()\n",
    "print df.shape\n",
    "df.drop(['architecturalstyletypeid','basementsqft', 'buildingclasstypeid', 'decktypeid', 'garagecarcnt',\n",
    "        'poolsizesum', 'pooltypeid10', 'pooltypeid2', 'pooltypeid7', 'roomcnt', 'yardbuildingsqft26', \n",
    "         'numberofstories', 'fireplaceflag', 'fullbathcnt', 'heatingorsystemtypeid','storytypeid', 'threequarterbathnbr', 'typeconstructiontypeid', \n",
    "         'structuretaxvaluedollarcnt', 'taxvaluedollarcnt', 'assessmentyear', 'landtaxvaluedollarcnt', \n",
    "         'taxamount', 'bathroomcnt'], axis=1, inplace=True)\n",
    "print df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "before_rogue = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(25, 25))\n",
    "temp = pd.merge(before_rogue,train,on='parcelid')\n",
    "hmap = sbn.heatmap(temp.corr('pearson'), annot=True, fmt=\".3f\", linewidths=.5, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fgure = hmap.get_figure()\n",
    "fgure.savefig('before_rogue_correlation_with_logerror.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "temp.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#going rogue\n",
    "df.drop(['propertycountylandusecode', 'propertylandusetypeid', 'propertyzoningdesc', 'censustractandblock', \n",
    "         'rawcensustractandblock', 'regionidcity', 'taxdelinquencyflag', 'taxdelinquencyyear', 'regionidneighborhood'],axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "merged = pd.merge(df,train,on = 'parcelid')\n",
    "# print merged.shape\n",
    "# merged.info(null_counts= True)  ##checked, there are 0 NaN values in the merged dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dates = []\n",
    "month = []\n",
    "for i in merged['transactiondate']:\n",
    "    dates.append(get_diff_month(datetime.strptime(i,'%Y-%m-%d'),datetime(2015,1,1)))\n",
    "    month.append(pd.to_datetime(i, format='%Y-%m-%d').month)\n",
    "\n",
    "# print month\n",
    "\n",
    "merged['transactiondate'] = dates\n",
    "merged['month_of_year'] = month"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Transaction date is defined as the number of months since 2015/1/1 after which the transaction was performed.\n",
    "We are removing the date information because in the prediction, only the year and month of the transaction are given"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print before_rogue.shape, merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Draw a heatmap with the numeric values in each cell\n",
    "f, ax = plt.subplots(figsize=(20, 20))\n",
    "sbn.heatmap(merged.corr('pearson'), annot=True, fmt=\"f\", linewidths=.5, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# month_v_error = merge_copy[['month_of_year','logerror']].plot.bar(figsize= (12,7))\n",
    "month_v_error =  merged[['month_of_year','logerror']].groupby(by='month_of_year').mean().plot.line(figsize= (7,7),\n",
    "                                                                                                   ylim =[0.0065,0.0195])\n",
    "# plt.title('Percentage of missing values vs. properties in data')\n",
    "# plt.xlabel('Properties of data')\n",
    "# plt.ylabel('Percentage of missing values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we plot the month of the year against the mean logerror, we see that the value of the bar graph reduces in the months of March-June before rising up again in July. However, this is not to be intepreted as a reduction in the error, it only means that the mean is lower, the logerror could be negative. We will have to plot the month against the absolute error to see if their model is more accurate during the specified months or does it just give negative error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "month_v_error = merged[['month_of_year','logerror']]\n",
    "month_v_error['logerror'] = abs(month_v_error['logerror'])\n",
    "month_v_error1 =  month_v_error[['month_of_year','logerror']].groupby(by='month_of_year').mean().plot.line(figsize= (7,7),\n",
    "                                                                                                           ylim = [0.0065,0.08])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As suspected, there is no correlation between the month of the year and the absolute value of logerror. We can still use the value of 'month_of_year' as a feature and see if it benefits the model in anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "age_v_error = merged[['age','logerror']].copy()\n",
    "age_v_error['age'] = [2015-i for i in age_v_error['age']]\n",
    "age_v_error.loc[:,'logerror'] = abs(month_v_error['logerror'])\n",
    "# age_v_error = age_v_error[age_v_error['logerror'] <  age_v_error['logerror'].quantile(0.995)]  # exclude outliers\n",
    "# age_v_error = age_v_error[age_v_error['logerror'] >  age_v_error['logerror'].quantile(0.005)]\n",
    "age_v_error1 =  age_v_error[['age','logerror']].groupby(by='age').mean().plot.line(figsize= (17,7))\n",
    "plt.xlabel('Year Built')\n",
    "plt.ylabel('Log Error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the mean absolute log error shows a reducing trend for properties that were more recently built when compared to the other older ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print merged.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print merged['actual_area'].isnull().sum() #here we find out that actual area is null for 661 values, so we drop those points\n",
    "\n",
    "merged.dropna(axis=0,how='any',inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to interpolate later if we have time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merged = pd.read_csv('./data/merge_copy.csv',index_col =0) #outliers are already removed (mostly :P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'parcelid', u'airconditioningtypeid', u'bedroomcnt',\n",
       "       u'buildingqualitytypeid', u'calculatedbathnbr', u'fips',\n",
       "       u'fireplacecnt', u'garagetotalsqft', u'hashottuborspa', u'latitude',\n",
       "       u'longitude', u'lotsizesquarefeet', u'poolcnt', u'regionidcounty',\n",
       "       u'regionidzip', u'unitcnt', u'yardbuildingsqft17', u'actual_area',\n",
       "       u'calculatedvalue', u'age', u'logerror', u'transactiondate',\n",
       "       u'month_of_year'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merge_copy = merged.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merged = merge_copy #shape should be (89597, 23)\n",
    "# merged['logerror'] = logerror"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merged = merged[merged['logerror'] <  merged['logerror'].quantile(0.995)]  # exclude outliers\n",
    "merged = merged[merged['logerror'] >  merged['logerror'].quantile(0.005)]\n",
    "\n",
    "#shape should be (88700, 23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logerror = merged['logerror']\n",
    "# invlogerror = [10**i for i in logerror]\n",
    "X_train, X_test, y_train, y_test = train_test_split(merged.drop(['logerror','parcelid'],axis=1), logerror, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(-1,1)) \n",
    "s_train = scaler.fit_transform(X_train)\n",
    "s_test = MinMaxScaler(feature_range=(-1,1)).fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "clf = LinearRegression(n_jobs=-1,normalize=True)\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "res = clf.predict(X_test)\n",
    "# res = [log(i,10) if i>0 else -log(i*-1,10) for i in res]\n",
    "# print clf.score(X_test,y_test)\n",
    "print mean_absolute_error(y_test,res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WORK FOR NEXT TIME,TRY TO HANDLE NEGATIVE VALUES IN LOGERROR SO THAT AFTER TAKING ANTILOG, WE CAN AGAIN TAKE LOG FOR SUBMISSION PURPOSES"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "score before removing outliers\n",
    "0.00314715026497\n",
    "\n",
    "score after removing outliers\n",
    "0.000527806351553"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0565932382901\n"
     ]
    }
   ],
   "source": [
    "print mean_absolute_error(y_test,res)\n",
    "# print mean_squared_error([log(i,10) if i>0 else -log(i*-1,10) for i in y_test],res)**0.5#,r2_score([log(i,10) for i in y_test],res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print mean_squared_error(y_test,res)**0.5"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "mean_r^2, r2_score before removing the outliers\n",
    "0.0262499375484 0.00314715026497\n",
    "\n",
    "mean_r^2, r2_score, mean_abs after removing the outliers\n",
    "0.00936501383416 0.000527806351553 0.0565932382901"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print merged.keys()\n",
    "print pd.DataFrame({'columns': list(set(merged.keys()) - set(['logerror','parcelid'])),'coeffs': clf.coef_}) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This information will be later used to find out the most important and the weakest attributes that are used for linear regression\n",
    "\n",
    "The best feature so far in the model is the regionidzip with the coefficient -1.075408e-02 and the weakest feature are both  yardbuildingsqft17 and hashottuborspa with the coefficient -1.274611e+11 and 1.274611e+11 respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# print res.mean(),y_test.mean()\n",
    "print pd.DataFrame(res).describe(),\"\\n\\n\",pd.DataFrame(y_test).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(res,y_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0566121462041\n"
     ]
    }
   ],
   "source": [
    "lasso = linear_model.Lasso(alpha = 0.0001,tol=0.000000001,max_iter=2147483647,warm_start = True,normalize=False)\n",
    "lasso.fit(X_train,y_train)\n",
    "print mean_absolute_error(lasso.predict(X_test),y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0566074191938\n"
     ]
    }
   ],
   "source": [
    "ridge = linear_model.Ridge(alpha = 2000,normalize=False, tol=0.000000001, max_iter=2147483647)\n",
    "ridge.fit(X_train,y_train)\n",
    "print mean_absolute_error(ridge.predict(X_test),y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####takes too long to train\n",
    "# enet = linear_model.ElasticNet(alpha = 0.00008,normalize=False,l1_ratio=0.5,tol=0.000000001,max_iter=2147483647,warm_start=True)\n",
    "# enet.fit(X_train,y_train)\n",
    "# print mean_absolute_error(enet.predict(X_test),y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### Something is absurdly wrong with this classifier or the code\n",
    "\n",
    "\n",
    "# sgd = linear_model.SGDRegressor(max_iter=2147483647)\n",
    "\n",
    "# param_grid = {\n",
    "# #     'penalty': ('l1','l2'),\n",
    "#     'alpha': [10,100,1000],\n",
    "#     'tol': [0.000000001],\n",
    "# #     'warm_start': [True,False] True was proved to give better result\n",
    "    \n",
    "# # start = datetime.now().time()\n",
    "# gcv = GridSearchCV(sgd,param_grid,scoring = make_scorer(mean_absolute_error,greater_is_better=False))\n",
    "# gcv.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr = svm.LinearSVR(C=100,tol=0.000001,max_iter=2147483647) # parameters not affecting the result\n",
    "svr.fit(X_train,y_train)\n",
    "print mean_absolute_error(svr.predict(X_test),y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C=10   0.057099199549\n",
    "C=1    0.057099199549"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "knr = KNR(n_jobs=-1)\n",
    "knr_param_grid = {\n",
    "    'n_neighbors' : [1800,2500,5000],\n",
    "    'leaf_size': [200,300,600],\n",
    "    'algorithm': ('auto', 'ball_tree', 'kd_tree', 'brute'),\n",
    "    \n",
    "    \n",
    "}\n",
    "knr_gcv = GridSearchCV(knr,knr_param_grid,scoring = make_scorer(mean_absolute_error,greater_is_better=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "          metric_params=None, n_jobs=-1, n_neighbors=5, p=2,\n",
       "          weights='uniform'),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'n_neighbors': [600, 1200, 1800], 'leaf_size': [300, 500, 800]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=make_scorer(mean_absolute_error, greater_is_better=False),\n",
       "       verbose=0)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knr_gcv.fit(s_train,y_train)\n",
    "# print mean_absolute_error(knr.predict(X_test),y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best = knr_gcv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print mean_squared_error(best.predict(s_test),y_test), best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.0568260301871 \n",
    "KNeighborsRegressor(algorithm='auto', leaf_size=500, metric='minkowski',\n",
    "          metric_params=None, n_jobs=-1, n_neighbors=625, p=2,\n",
    "          weights='uniform')\n",
    "with scaled input \n",
    "0.0567777979104\n",
    "same knr as above\n",
    "\n",
    "using scaled only now\n",
    "0.0567225844231 KNeighborsRegressor(algorithm='auto', leaf_size=300, metric='minkowski',\n",
    "          metric_params=None, n_jobs=-1, n_neighbors=1800, p=2,\n",
    "          weights='uniform')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svr = svm.LinearSVR(tol=0.0000000001,max_iter = 2147483647)\n",
    "svr_param_grid = {\n",
    "    'C' : [1,10,100,1000],\n",
    "    'loss': ('epsilon_insensitive','squared_epsilon_insensitive')    \n",
    "}\n",
    "\n",
    "svr_gcv = GridSearchCV(svr,svr_param_grid,scoring = make_scorer(mean_absolute_error,greater_is_better=False),n_jobs=-1)\n",
    "\n",
    "svr_gcv.fit(s_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print \"yolo!!\"\n",
    "# print mean_absolute_error(svr.predict(s_test),y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(88700, 23)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAE of default LinearSVR 0.0530913020565"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
